import pandas as pd
import matplotlib.pyplot as plt

lin_reg_data = pd.read_csv("./linear_regression_data.csv")

def loss_function(m,b):
    #total error is equal to the average of the means squared error, ie 1/n * sum( (y - y_prediction)^2 )
    error = 0
    for coord in range(len(lin_reg_data["x"])):
        y = lin_reg_data.loc[coord, "y"]
        x = lin_reg_data.loc[coord, "x"]
        y_predict = m * x + b
        error += (y - y_predict)**2
    
    error /= float(len(lin_reg_data.x)-1)
    return error

def gradient_descent(current_m, current_b, epochs, learning_rate):
    #epochs is the number of iternations for gradient descent
    #learning rate is the size of our jumps
    #by what both m and b need to change
    delta_m = 0
    delta_b = 0
    points_amount = len(lin_reg_data.x)
    loss_list = []
    for _ in range(epochs):
        for coord in range(points_amount):
            y = lin_reg_data.loc[coord, "y"]
            x = lin_reg_data.loc[coord, "x"]
            y_predict = current_m * x + current_b

            delta_m += x*(y - y_predict)
            delta_b += y - y_predict

        #these formulas were generated by taking the derivative of the loss fxn
        #in terms of m & b respectively
        delta_m *= -(2/points_amount)
        delta_b *= -(2/points_amount)

        #updating the m values according to the deltas
        current_m -= delta_m * learning_rate
        current_b -= delta_m * learning_rate

        loss_list.append(loss_function(current_m, current_b))
    
    return current_m,current_b, loss_list

def predict_ys():
    y_list = []
    for x in lin_reg_data.x:
        y_list.append(m*x + b)
    return y_list

m = 1
b = 0

epochs = 1000
learning_rate = 0.0001
error_list = []

#learning process
m,b, error_list = gradient_descent(m,b, epochs, learning_rate)
#plotting csv points
plt.scatter(lin_reg_data["x"], lin_reg_data["y"])
#plotting predicted line
plt.plot(lin_reg_data.x, predict_ys(), color="black")    
plt.show()


